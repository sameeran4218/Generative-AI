{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d87f772",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf2deb",
   "metadata": {},
   "source": [
    "## Response from APIs vs LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Response from API directly\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client= anthropic.Anthropic()\n",
    "# need to define api-key first\n",
    "message=client.messages.create(\n",
    "    model='claude-3-7-sonnet-20250219',\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=' You are a knowledgable physicist ai model',\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':[\n",
    "                {\n",
    "                    'type':'text',\n",
    "                    'text':'What is a blackhole?'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)\n",
    "\n",
    "# 2. Response from LangChain\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "model= ChatAnthropic(model='claude-3-7-sonnet-20250219', temperature=0)\n",
    "response=model.invoke(\"Explain the theory of relativity in simple terms.\").content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2311fa",
   "metadata": {},
   "source": [
    "Thus as we can see while using APIS directly we have to manage a lot of things on our own like message formatting, handling different types of responses etc. But when we use LangChain it abstracts away all these complexities and provides a simple interface to interact with different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7fe34",
   "metadata": {},
   "source": [
    "## 1. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b097d",
   "metadata": {},
   "source": [
    "LLMs are general purpose models that can be used for text generation, summarization, translation, and more. Chat models are specifically designed for conversational tasks and can handle multi-turn dialogues effectively. Similarly we can also use Embedding models with Langchain to convert text into vector representations for tasks like semantic search and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e24920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model from Groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "result=llm.invoke(\"Explain the theory of relativity in simple terms.\").content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1e176",
   "metadata": {},
   "source": [
    "This is a simple demo of how we can get responses from a language model of Groq using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Model from Groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "# We need to explicitly define messages such as SystemMessage, HumanMessage etc. to get multi-turn conversations.\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following English text to French: 'Hello, how are you?'\")\n",
    "]\n",
    "\n",
    "result=llm.invoke(messages).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb3570",
   "metadata": {},
   "source": [
    "An example of how to use Conversational chat models from Groq via LangChain for a multi-turn conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b237fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a locally downloaded Embedding model with LangChain\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import sentence_transformers\n",
    "embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text='My name is Sameeran'\n",
    "vector=embedding.embed_query(text)\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f818e",
   "metadata": {},
   "source": [
    "An example of how we can download and run embedding models locally using LangChain and Huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76531d1",
   "metadata": {},
   "source": [
    "## 2. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0efd2f",
   "metadata": {},
   "source": [
    "Static Prompts are fixed templates that do not change based on input. They are useful for simple tasks where the same prompt structure can be reused. Dynamic Prompts, on the other hand, can change based on input variables or context. They are more flexible and can be tailored to specific tasks or user inputs. Static prompt give much control to user which can lead to llm hallucinations while dynamic prompts can help in reducing such issues by providing more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating single-turn dynamic prompt with LangChain\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "template=PromptTemplate(template='''Describe the given {product} in a creative way.''', input_variables=['product'])\n",
    "prompt=template.invoke({'product':'IPhone 15 Pro Max'})\n",
    "result=llm.invoke(prompt).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading predefined prompt templates with LangChain\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate,load_prompt\n",
    "load_dotenv()\n",
    "\n",
    "template=PromptTemplate(template='''Describe the given {product} in a creative way.''', input_variables=['product'])\n",
    "template.save('template.json')\n",
    "loaded_template=load_prompt('template.json') # file path\n",
    "prompt=loaded_template.invoke({'product':'Samsung Galaxy S23 Ultra'})\n",
    "result=llm.invoke(prompt).content\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf08ac",
   "metadata": {},
   "source": [
    "Messages like System, Human, AI can be used to create multi-turn conversations with a conversational model. They can be used to explicitly define the roles of different participants in the conversation. Also allow for easy storage and retrieval of chat histories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a1d28",
   "metadata": {},
   "source": [
    "Human Message is used to represent the input from the user.\n",
    "AI Message is used to represent the response from the AI model.\n",
    "System Message is used to set the context or behavior of the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Messages to create static multi-turn conversations with LangChain\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "load_dotenv()\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"Translate the following English text to French: 'Hello, how are you?'\")\n",
    "]\n",
    "result=llm.invoke(messages).content\n",
    "# We can add the result as AIMessage to continue the conversation\n",
    "messages.append(AIMessage(content=result))\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ChatPromptTemplate for dynamic multi-turn conversations with LangChain.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "load_dotenv()\n",
    "\n",
    "# assigning roles and their content in chat prompt template\n",
    "chat_template= ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful {domain} assistant.\"),\n",
    "    (\"human\",\"Explain {topic} in simple terms.\")\n",
    "])\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "prompt=chat_template.invoke({'domain':'technology','topic':'machine learning'})\n",
    "result=llm.invoke(prompt).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32df412",
   "metadata": {},
   "source": [
    "Message Placeholder is a special placeholder used inside ChatPromptTemplate to dynamically insert chat history or a list of messages at runtime. It allows for more flexible and context-aware conversations by incorporating previous interactions into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92099494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagePlaceholder\n",
    "load_dotenv()\n",
    "\n",
    "chat_template=ChatPromptTemplate.from_messages([\n",
    "    ('system','You are a helpful customer support assistant.'),\n",
    "    MessagePlaceholder(variable_name='chat_history'),\n",
    "    ('human','{query}')\n",
    "])\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "# create chat history where we save chat history in a text file\n",
    "chat_history=[]\n",
    "with open('chat_history.txt') as f:\n",
    "    chat_history.extend(f.readlines())\n",
    "\n",
    "# create prompt to dynamically insert chat history with help of MessagePlaceholder\n",
    "prompt=chat_template.invoke({'chat_history':chat_history,'query':'How can I reset my password?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad8bb4",
   "metadata": {},
   "source": [
    "## 3. Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a4366",
   "metadata": {},
   "source": [
    "Structured Output refers to the practice of formatting the output from language models in a predefined structure, such as JSON rather than plain text.This makes the output easier to parse and use programmatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e410ac7",
   "metadata": {},
   "source": [
    "i. Getting Structured Outputs for models that support it natively. Methods are using TypedDict, Pydanctic or Json Schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using TypedDict which allows us to define the data types of the keys and values in a dictionary. \n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict,Annotated\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "# create an output schema using the TypedDict class\n",
    "class Format(TypedDict):\n",
    "    # here Annotated is used to add descriptions to the fields\n",
    "    summary:Annotated[str,'A brief summary of the text']\n",
    "    sentiment:Annotated[str,' The sentiment of the text, can be Positive, Negative or Neutral']\n",
    "    key_themes:Annotated[list[str],'A list of key themes discussed in the text']\n",
    "\n",
    "# now use this schema and pass it to the llm\n",
    "llm=llm.with_structured_output(Format)\n",
    "response=llm.invoke(\"Analyze the following text: 'LangChain is a powerful framework for building applications with language models. It simplifies the process of integrating various components and allows developers to create complex workflows easily.'\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using Pydantic library to define structured output schema. Used when you want data validation, need to define default values or optional fields in case llm misses them.\n",
    "\n",
    "from typing import Optional\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "# create an output schema using the BaseModel class from Pydantic\n",
    "class Format(BaseModel):\n",
    "    key_themes: list[str]=Field(description=\"A list of key themes discussed in the text\")\n",
    "    summary: str=Field(description=\"A brief summary of the text\")\n",
    "    pros: Optional[list[str]]=Field(default=None,description=\"A list of pros of the text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41080e",
   "metadata": {},
   "source": [
    "ii. Using Output Parsers to convert unstructured text outputs into structured formats for LLMs that do not support structured outputs natively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using StringOutputParser to parse unstructured text output as plain text.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "template=PromptTemplate(template='''Describe the given {product} in a creative way.''', input_variables=['product'])\n",
    "\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'product':'Google Pixel 8 Pro'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Using JsonOutputParser to parse unstructured text output as JSON. This parser just returns the output in JSON format but it doesnt enforce any schema for the llm to follow.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=JsonOutputParser()\n",
    "template=PromptTemplate(template='Provide the details of the given {product}. \\n {format_intructions}',\n",
    "                        input_variables=['product'],\n",
    "                        partial_variables={'format_intructions':parser.get_format_instructions()})\n",
    "# this partial variable allows us to include the return type of the output of the llm according to the parser used. for eg here it is JSON.\n",
    "\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'product':'Google Pixel 8 Pro'})\n",
    "print(result)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009f3b4",
   "metadata": {},
   "source": [
    "Using Json Output Parser we dont have control over the structure of the JSON output. for eg we cannot determine what the keys will be in the output JSON and without knowing the keys we cannot access the values. This can be avoided by using StructuredOutputParser where we can create a predefined schema with known key values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. StructuredOutputParser helps to extract a structured JSON data from LLM based on a predefined schema but doesnt provide data validation.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "schema= [\n",
    "    ResponseSchema(name='summary', description='A brief summary of the text'),\n",
    "    ResponseSchema(name='sentiment', description='The sentiment of the text, can be Positive, Negative or Neutral'),\n",
    "    ResponseSchema(name='key_themes', description='A list of key themes discussed in the text')\n",
    "]\n",
    "# passing the schema to the parser\n",
    "parser=StructuredOutputParser.from_response_schemas(schema)\n",
    "template=PromptTemplate(template='''Analyze the following text: {text}.\\n{format_instructions}''',\n",
    "                        input_variables=['text'],\n",
    "                        partial_variables={'format_instructions':parser.get_format_instructions()})\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'text':\"LangChain is a powerful framework for building applications with language models. It simplifies the process of integrating various components and allows developers to create complex workflows easily.\"})\n",
    "print(result)\n",
    "\n",
    "# Note. this is an example, actually this LLM doesnt support StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PydanticOutputParser allows us to define and enforce a Pydantic model as the output schema for parsing the LLM output. It ensures data valdation according to the defined schema.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "\n",
    "# create an output schema using the BaseModel class from Pydantic\n",
    "class Format(BaseModel):\n",
    "    key_themes: list[str]=Field(description=\"A list of key themes discussed in the text\")\n",
    "    summary: str=Field(description=\"A brief summary of the text\")\n",
    "    pros: Optional[list[str]]=Field(default=None,description=\"A list of pros of the text\")\n",
    "\n",
    "# passing the pydantic model to the parser\n",
    "parser=PydanticOutputParser(pydantic_object=Format)\n",
    "template=PromptTemplate(template='''Analyze the following text: {text}.\\n{format_intructions}''',\n",
    "                        input_variables=['text'],\n",
    "                        partial_variables={'format_intructions':parser.get_format_instructions()})\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'text':\"LangChain is a powerful framework for building applications with language models. It simplifies the process of integrating various components and allows developers to create complex workflows easily.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fa709",
   "metadata": {},
   "source": [
    "## 4. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431f00e",
   "metadata": {},
   "source": [
    "A chain is a structured pipeline that links multiple components together to perform a task in a defined sequence. Chains can include components like language models, prompt templates, and output parsers. They help in breaking down complex tasks into smaller, manageable steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e852ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple Chain\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "template=PromptTemplate(template='''Describe the given {product} in a creative way.''', input_variables=['product'])\n",
    "\n",
    "# creating a chain by linking prompt template, llm and output parser\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'product':'OnePlus 11'})\n",
    "print(result)\n",
    "chain.get_graph().print_ascii() # to visualize the chain structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79631b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sequential Chain (using multiple prompt templates and llms )\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "\n",
    "template1=PromptTemplate(template='''Describe the given {product} in a detailed, creative way.''', input_variables=['product'])\n",
    "template2=PromptTemplate(template='''Based on the following product description, generate a catchy marketing slogan: {description}''', input_variables=['description'])\n",
    "\n",
    "# creating a sequential chain by linking multiple prompt templates and llms\n",
    "chain=template1|llm|parser|template2|llm|parser\n",
    "result=chain.invoke({'product':'OnePlus 11'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parallel Chain (using RunnableParallel to run multiple branches in parallel and combine their outputs)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model='llama-3.3-70b-versatile', temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template1 = PromptTemplate(template=\"generate short and simple notes from the following text: {text}\", input_variables=['text'])\n",
    "template2 = PromptTemplate(template=\"generate question-answer pairs from the following text: {text}\", input_variables=['text'])\n",
    "template3 = PromptTemplate(template=\"Merge these notes: {notes} and these QA pairs: {qa_pairs} into a comprehensive study guide.\", input_variables=['notes', 'qa_pairs'])\n",
    "\n",
    "# 1. Define the parallel branches\n",
    "parallel_chain = RunnableParallel({\n",
    "    'notes': template1 | llm | parser,\n",
    "    'qa_pairs': template2 | llm | parser\n",
    "})\n",
    "\n",
    "# 2. Define the full chain\n",
    "# The output of parallel_chain is a dict: {'notes': '...', 'qa_pairs': '...'}\n",
    "# This dict matches the input requirements of template3 perfectly.\n",
    "chain = parallel_chain | template3 | llm | parser\n",
    "\n",
    "result = chain.invoke({'text': 'LangChain is a powerful framework for building applications with language models. It simplifies the process of integrating various components and allows developers to create complex workflows easily.'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Conditional Chain (using RunnableBranch to choose different branches based on input)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model='llama-3.3-70b-versatile', temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# To ensure the LLM output can be used for branching, we define a Pydantic model for sentiment classification\n",
    "class FeedBack(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative']=Field(description=\"The sentiment of the review, can be positive or negative\")\n",
    "sentiment_parser=PydanticOutputParser(pydantic_object=FeedBack)\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following review as Positive or Negative.\n",
    "    {format_instructions}\n",
    "    Review: {review}\"\"\",\n",
    "    input_variables=['review'], \n",
    "    partial_variables={'format_instructions': sentiment_parser.get_format_instructions()}\n",
    ")\n",
    "prompt2=PromptTemplate(template='''Generate a response to this {review} positive customer review. Do not say anything except the response.''')\n",
    "prompt3=PromptTemplate(template='''Generate a constructive response to address this {review} negative customer review.  Do not say anything except the response.''')\n",
    "\n",
    "classifier_chain=prompt1|llm|sentiment_parser\n",
    "\n",
    "# Create conditional branches based on sentiment                               \n",
    "branch_chain=RunnableBranch(\n",
    "    (lambda x:x.sentiment=='positive', prompt2|llm|parser),                  # executes if based on the input x, condition is True\n",
    "    (lambda x:x.sentiment=='negative', prompt3|llm|parser),\n",
    "    RunnableLambda(lambda x:\"Could not determine sentiment.\")                   # here we can use a default chain if none of the conditions match. Here we dont have a default chain so we turn a lambda function with response into a chain using RunnableLambda.\n",
    ")\n",
    "\n",
    "chain=classifier_chain|branch_chain\n",
    "result=chain.invoke({'review':'The product quality is excellent and I am very satisfied with my purchase!'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f1f001",
   "metadata": {},
   "source": [
    "## 5. Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88661d",
   "metadata": {},
   "source": [
    "Runnables are abstractions in LangChain that represent executable components or workflows. They can encapsulate various operations, such as calling language models, processing data, or chaining multiple steps together. Runnables provide a unified interface for executing these operations, making it easier to build and manage complex workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf1680",
   "metadata": {},
   "source": [
    "i. Task Specific Runnables: These are core LangChain components that perform specific tasks, such as invoking a language model or processing data. Examples include LLMs, Chat Models, Prompt Templates, and Output Parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e3a32",
   "metadata": {},
   "source": [
    "ii. Runnable Primitives: These are fundamental building blocks that help connect different Task Specific Runnables together to create complex workflows. Examples include RunnableSequence, RunnableBranch, RunnableLambda, RunnableParallel, RunnablePassthrough etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RunnableSequence: This primitive allows you to chain multiple Runnables together in a sequential manner. The output of one Runnable becomes the input for the next Runnable in the sequence.\n",
    "\n",
    "# we can use the pipe operator '|' to create a RunnableSequence\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "prompt=PromptTemplate(template='''Write a funny joke about {topic}''',input_variables=['topic'])\n",
    "\n",
    "# creating a runnable sequence by linking prompt template, llm and output parser\n",
    "chain=RunnableSequence(prompt,llm,parser)\n",
    "result=chain.invoke({'topic':'computers'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RunnableParallel: This primitive allows you to run multiple Runnables in parallel. Each Runnable operates independently, recieves the same input and their outputs are collected into a dictionary.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "prompt1=PromptTemplate(template='''Write a tweet about {topic}''',input_variables=['topic'])\n",
    "prompt2=PromptTemplate(template='''Craft a linkedin post about {topic}''',input_variables=['topic'])\n",
    "# creating a runnable parallel by linking multiple prompt templates, llms and output parsers\n",
    "chain=RunnableParallel({\n",
    "    'tweet':prompt1|llm|parser,\n",
    "    'linkedin':prompt2|llm|parser\n",
    "})\n",
    "result=chain.invoke({'topic':'artificial intelligence'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RunnablePassThrough: This primitive simply passes the input it receives directly to its output without any modifications or processing. It acts as a transparent conduit for data.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "prompt=PromptTemplate(template='''Write a funny joke about {topic}''',input_variables=['topic'])\n",
    "\n",
    "# creating a runnable sequence by linking prompt template, llm and output parser\n",
    "joke_chain=RunnableSequence(prompt,llm,parser)\n",
    "\n",
    "# Now we wish to explain the joke generated above, and print both the joke and its explanation, thus we use RunnablePassThrough to print the joke as it is.\n",
    "prompt2=PromptTemplate(template='''Explain the following joke: {joke}''',input_variables=['joke'])\n",
    "explain_chain=RunnableParallel({\n",
    "    'joke':RunnablePassthrough(),\n",
    "    'explaination':RunnableSequence(prompt2,llm,parser)\n",
    "})\n",
    "\n",
    "chain=RunnableSequence(joke_chain,explain_chain)\n",
    "result=chain.invoke({'topic':'Artificial Intelligence'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f8e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the elephant invite the ant to his party?\\n\\nBecause he wanted to have a trunk-load of fun and an ant-icipated good time! (get it?)', 'word_count': 26}\n"
     ]
    }
   ],
   "source": [
    "# 4. RunnableLambda: This primitive allows you to apply custom python functions as Runnables within your ai pipelines.\n",
    "\n",
    "# we will create a chain that generates a joke and then counts the number of words in the joke using a custom python function.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "prompt1=PromptTemplate(template='''Write a funny joke about {topic}''',input_variables=['topic'])\n",
    "joke_chain=RunnableSequence(prompt1,llm,parser)\n",
    "\n",
    "# custom function to count words in a text\n",
    "def count_words(text:str)->int:\n",
    "    return len(text.split())\n",
    "\n",
    "count_chain=RunnableParallel({\n",
    "    'joke':RunnablePassthrough(),\n",
    "    'word_count':RunnableLambda(count_words) # using custom function as a runnable\n",
    "})\n",
    "\n",
    "chain=RunnableSequence(joke_chain,count_chain)\n",
    "result=chain.invoke({'topic':'Elephant and Ant'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RunnableBranch: This primitive allows you to conditionally route data into different chains or runnables based on custom logic, essentially helping to build conditional chains.\n",
    "\n",
    "# we will create a chain such that based on a topic, llm will generate a report and if report >500 words we shall summarize it else leave it as it is.\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableBranch\n",
    "load_dotenv()\n",
    "\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "prompt1=PromptTemplate(template='''Generate a detailed report on the topic: {topic}''',input_variables=['topic'])\n",
    "report_chain=RunnableSequence(prompt1,llm,parser)\n",
    "\n",
    "# function to check if report length >500 words\n",
    "def count(text:str)->bool:\n",
    "    return len(text.split())>500\n",
    "prompt2=PromptTemplate(template='''Summarize the following report: {report} within 500 words''',input_variables=['report'])\n",
    "summarize_chain=RunnableBranch(\n",
    "    (count, RunnableSequence(prompt2,llm,parser)),  # if report >500 words, summarize it\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "chain=RunnableSequence(report_chain,summarize_chain)\n",
    "result=chain.invoke({'topic':'Climate Change and its Impact on Global Ecosystems'})\n",
    "print(result)\n",
    "print(count(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061434ee",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) is a technique that combines retrieval of relevant documents with generation of responses using language models to provide more accurate and context-aware answers.\n",
    "In LangChain, the main components involved in RAG are Document Loaders, Text Splitters, Vector Stores, Retrievers, and LLMs/Chat Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1c67b",
   "metadata": {},
   "source": [
    "## 6. Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410c9e6",
   "metadata": {},
   "source": [
    "Document Loaders are components in Langchain used to load data from various sources into a standardized format (usually as Document objects) for further processing. Each document has two main attributes: page_content (the actual text content of the document) and metadata (additional information about the document, such as source, author, date, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92132bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text Loaders - reads plain text files (.txt) and loads them as Document objects.\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('ai.txt',encoding='utf-8') \n",
    "documents=loader.load()\n",
    "# converts to a list of Document objects \n",
    "print(documents,type(documents))\n",
    "print(documents[0],type(documents[0]))\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()\n",
    "llm=ChatGroq(model='llama-3.3-70b-versatile',temperature=0)\n",
    "parser=StrOutputParser()\n",
    "template=PromptTemplate(template='''Summarize the following text: {text} in 100 words''', input_variables=['text'])\n",
    "chain=template|llm|parser\n",
    "result=chain.invoke({'text':documents[0].page_content})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7552a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PyPDF Loader - reads PDF files (.pdf) on a page-wise basis and loads each page as Document object, doesnt handle scanned PDFs.\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('sample.pdf')\n",
    "documents=loader.load()\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c305846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Web Page Loader - loads web pages from a given URL using BeautifulSoup, doesnt handle javascript rendered content (use SeleniumURLLoader)\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "documents = loader.load()\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703f5ce",
   "metadata": {},
   "source": [
    "Similarly we can use the CSVLoader to load csv files row-wise as Document objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14246bb5",
   "metadata": {},
   "source": [
    "## 7. Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a4dc4",
   "metadata": {},
   "source": [
    "Text Splitters perform breaking of large documents into smaller manageable chunks that LLMs can process effectively.\n",
    "Text Splitting improves the embedding quality by producing accurate vectors and enhances the semantic search results as well. Provides for memory efficient processing of large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99620762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Length-Based Text Splitter - splits text into chunks based on a specified maximum length (in characters or tokens) and an optional overlap between chunks.\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('sample.pdf')\n",
    "documents=loader.load()\n",
    "splitter=CharacterTextSplitter(chunk_size=200,chunk_overlap=20,separator=' ')\n",
    "chunks=splitter.split_documents(documents)\n",
    "# you can also use split_text() method to split plain text\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Structure Based Text Splitter - splits text into chunks based on its inherent structure like paragraphs, sections, headings etc.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('sample.pdf')\n",
    "documents=loader.load()\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50,separators=['\\n\\n','\\n','.','!','?',' ', ''])\n",
    "chunks=splitter.split_documents(documents)\n",
    "print(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bacb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Based on Language - splits text into chunks based on the specific language characteristics, for eg: python, markdown etc.\n",
    "\n",
    "text='''\n",
    "class Node:\n",
    "    def __init__(self, data): \n",
    "        self.data = data\n",
    "        self.next = None'''\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter=RecursiveCharacterTextSplitter.from_language(language='python',chunk_size=100,chunk_overlap=10)\n",
    "chunks=splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Semantic Text Splitter - splits text into semantically coherent chunks using embeddings to capture the meaning of the text.\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "pages = loader.load()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings, \n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "semantic_docs = text_splitter.split_documents(pages)\n",
    "print(semantic_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d2734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
